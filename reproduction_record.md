# 复现记录
## 1 管他什么先把代码跑起来再说
### 内容概括：
处理SMD数据集，然后使用20%的数据集训练，代码跑起来了。
### 运行内容：
① 创建虚拟环境 python=3.7

② 下载包
```
pip3 install torch==1.8.1+cpu torchvision==0.9.1+cpu torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html
pip3 install -r requirements.txt
```

③ 修改 dgl 版本 [dgl 下载链接](https://conda.anaconda.org/dglteam/linux-64)
：改为`0.8.1`（这个应该和torch==1.8.1有关）py37（对应python3.7）。
具体方法为下载后解压，把`lib/python3.7/site-packages`下面的`dgl-0.8.1-py3.7.egg-info`和
`dgl`复制到虚拟环境下的`/home/用户名/anaconda3/envs/虚拟环境名/lib/python3.7/site-packages`中。

④ 在 [plotting.py](src%2Fplotting.py) 中增加一行`import scienceplots`

⑤ 处理数据和训练
```
python3 preprocess.py SMD
python3 main.py --model 'TranAD' --dataset 'SMD' --retrain --less
```

### 运行结果：
```
Slicing dataset to 20%
Creating new model: TranAD
Training TranAD on SMD
Epoch 0,        L1 = 0.15016913696527226                                                                                                                                     
Epoch 1,        L1 = 0.0753215806853551                                                                                                                                      
Epoch 2,        L1 = 0.011504363452483395                                                                                                                                    
Epoch 3,        L1 = 0.0063193603349331615                                                                                                                                   
Epoch 4,        L1 = 0.005579655661899213                                                                                                                                    
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:05<00:00,  1.07s/it]
Training time:     5.3328 s
Testing TranAD on SMD
          f1  precision  recall      TP       TN      FP      FN   ROC/AUC   threshold
0   0.518060   0.349586     1.0  2660.0  20870.0  4949.0     0.0  0.904160    0.015198
1   0.617575   0.446737     1.0  1732.0  24602.0  2145.0     0.0  0.959902    0.047190
2   0.377887   0.232962     1.0  1275.0  23006.0  4198.0     0.0  0.922842    0.018925
3   0.339997   0.204819     1.0  1275.0  22254.0  4950.0     0.0  0.909021    0.015299
4   0.000000   0.000000     0.0     0.0  28479.0     0.0     0.0  0.000000    0.000862
5   0.000000   0.000000     0.0     0.0  27925.0     0.0   554.0  0.500000    0.787496
6   0.991938   0.984014     1.0   554.0  27916.0     9.0     0.0  0.999839    0.086875
7   0.000000   0.000000     0.0     0.0  28479.0     0.0     0.0  0.000000    0.000415
8   0.000000   0.000000     0.0     0.0  25812.0     0.0  2667.0  0.500000    9.530124
9   0.000000   0.000000     0.0     0.0  25819.0     0.0  2660.0  0.500000  105.308786
10  0.000000   0.000000     0.0     0.0  27203.0     1.0  1275.0  0.499982    0.421102
11  0.994759   0.989583     1.0  2660.0  25791.0    28.0     0.0  0.999458    0.055127
12  0.991445   0.983045     1.0  2667.0  25766.0    46.0     0.0  0.999109    0.005086
13  0.968756   0.939415     1.0  2667.0  25640.0   172.0     0.0  0.996668    0.008657
14  0.951476   0.907452     1.0  2667.0  25540.0   272.0     0.0  0.994731    0.021677
15  0.000000   0.000000     0.0     0.0  27203.0     1.0  1275.0  0.499982    0.430459
16  0.000000   0.000000     0.0     0.0  28479.0     0.0     0.0  0.000000    0.000745
17  0.000000   0.000000     0.0     0.0  28479.0     0.0     0.0  0.000000    0.000280
18  0.161703   0.087964     1.0   554.0  22181.0  5744.0     0.0  0.897153    0.016781
19  0.171569   0.093835     1.0   554.0  22575.0  5350.0     0.0  0.904208    0.017966
20  0.180808   0.099390     1.0   554.0  22905.0  5020.0     0.0  0.910116    0.022866
21  0.180366   0.099123     1.0   554.0  22890.0  5035.0     0.0  0.909848    0.015032
22  0.000000   0.000000     0.0     0.0  28477.0     2.0     0.0  0.000000    0.545734
23  0.996398   0.992832     1.0   554.0  27921.0     4.0     0.0  0.999928    0.002208
24  0.349935   0.212076     1.0  1275.0  22467.0  4737.0     0.0  0.912936    0.003380
25  0.953523   0.911184     1.0   554.0  27871.0    54.0     0.0  0.999033    0.001051
26  0.999995   1.000000     1.0   554.0  27925.0     0.0     0.0  1.000000    0.000693
27  0.353281   0.214538     1.0  1275.0  22536.0  4668.0     0.0  0.914204    0.015422
28  0.999995   1.000000     1.0   554.0  27925.0     0.0     0.0  1.000000    0.001224
29  0.134400   0.072042     1.0   554.0  20789.0  7136.0     0.0  0.872229    0.000520
30  0.179898   0.098840     1.0   554.0  22874.0  5051.0     0.0  0.909561    0.009638
31  0.231265   0.130753     1.0   554.0  24242.0  3683.0     0.0  0.934056    0.011302
32  0.999995   1.000000     1.0   554.0  27925.0     0.0     0.0  1.000000    0.000729
33  0.999995   1.000000     1.0   554.0  27925.0     0.0     0.0  1.000000    0.001747
34  0.176600   0.096853     1.0   554.0  22759.0  5166.0     0.0  0.907502    0.016407
35  0.176067   0.096532     1.0   554.0  22740.0  5185.0     0.0  0.907162    0.016189
36  0.000000   0.000000     0.0     0.0  28479.0     0.0     0.0  0.000000    0.001353
37  0.000000   0.000000     0.0     0.0  28479.0     0.0     0.0  0.000000    0.001376
{'FN': 7,
 'FP': 712,
 'Hit@100%': 0.4630422781253725,
 'Hit@150%': 0.5950907922224007,
 'NDCG@100%': 0.4699587346480527,
 'NDCG@150%': 0.5487333986956207,
 'ROC/AUC': 0.984895629329458,
 'TN': 25100,
 'TP': 2660,
 'f1': 0.880935618324324,
 'precision': 0.7888493452287979,
 'recall': 0.9973753243442994,
 'threshold': 0.03380556337847741}
 ```